{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from mamba_ssm import Mamba2\n",
    "import random\n",
    "random.seed(690)\n",
    "np.random.seed(690)\n",
    "torch.manual_seed(689)\n",
    "torch.cuda.manual_seed(609)\n",
    "torch.cuda.manual_seed_all(679)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 32/32 [01:14<00:00,  2.33s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "Train Loss: 1.7340\n",
      "Train: Accuracy: 0.5180, Precision: 0.3816, Recall: 0.5180, F1: 0.4093\n",
      "Val: Accuracy: 0.4950, Precision: 0.3877, Recall: 0.4950, F1: 0.4158\n",
      "Learning Rate: 0.001\n",
      "New best model saved with accuracy: 0.4950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]\n",
      "Train Loss: 1.1380\n",
      "Train: Accuracy: 0.5840, Precision: 0.6530, Recall: 0.5840, F1: 0.4964\n",
      "Val: Accuracy: 0.5100, Precision: 0.6326, Recall: 0.5100, F1: 0.4602\n",
      "Learning Rate: 0.001\n",
      "New best model saved with accuracy: 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 32/32 [00:15<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]\n",
      "Train Loss: 0.9320\n",
      "Train: Accuracy: 0.7420, Precision: 0.7877, Recall: 0.7420, F1: 0.7253\n",
      "Val: Accuracy: 0.6250, Precision: 0.6364, Recall: 0.6250, F1: 0.5894\n",
      "Learning Rate: 0.001\n",
      "New best model saved with accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 32/32 [00:15<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]\n",
      "Train Loss: 0.6693\n",
      "Train: Accuracy: 0.8700, Precision: 0.8789, Recall: 0.8700, F1: 0.8619\n",
      "Val: Accuracy: 0.6750, Precision: 0.6792, Recall: 0.6750, F1: 0.6622\n",
      "Learning Rate: 0.0008\n",
      "New best model saved with accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 32/32 [00:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]\n",
      "Train Loss: 0.3497\n",
      "Train: Accuracy: 0.8720, Precision: 0.8950, Recall: 0.8720, F1: 0.8661\n",
      "Val: Accuracy: 0.6700, Precision: 0.7038, Recall: 0.6700, F1: 0.6490\n",
      "Learning Rate: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]\n",
      "Train Loss: 0.3345\n",
      "Train: Accuracy: 0.8420, Precision: 0.8762, Recall: 0.8420, F1: 0.8402\n",
      "Val: Accuracy: 0.6700, Precision: 0.6787, Recall: 0.6700, F1: 0.6339\n",
      "Learning Rate: 0.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 32/32 [00:18<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]\n",
      "Train Loss: 0.2079\n",
      "Train: Accuracy: 0.9840, Precision: 0.9841, Recall: 0.9840, F1: 0.9838\n",
      "Val: Accuracy: 0.6950, Precision: 0.6777, Recall: 0.6950, F1: 0.6833\n",
      "Learning Rate: 0.00064\n",
      "New best model saved with accuracy: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]\n",
      "Train Loss: 0.0999\n",
      "Train: Accuracy: 0.9920, Precision: 0.9921, Recall: 0.9920, F1: 0.9919\n",
      "Val: Accuracy: 0.6950, Precision: 0.6825, Recall: 0.6950, F1: 0.6781\n",
      "Learning Rate: 0.00064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 32/32 [00:15<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]\n",
      "Train Loss: 0.0766\n",
      "Train: Accuracy: 0.9940, Precision: 0.9942, Recall: 0.9940, F1: 0.9940\n",
      "Val: Accuracy: 0.7150, Precision: 0.7041, Recall: 0.7150, F1: 0.7020\n",
      "Learning Rate: 0.00064\n",
      "New best model saved with accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]\n",
      "Train Loss: 0.0543\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7300, Precision: 0.7362, Recall: 0.7300, F1: 0.7291\n",
      "Learning Rate: 0.0005120000000000001\n",
      "New best model saved with accuracy: 0.7300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 32/32 [00:17<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]\n",
      "Train Loss: 0.0160\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7550, Precision: 0.7621, Recall: 0.7550, F1: 0.7555\n",
      "Learning Rate: 0.0005120000000000001\n",
      "New best model saved with accuracy: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]\n",
      "Train Loss: 0.0088\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7300, Precision: 0.7238, Recall: 0.7300, F1: 0.7237\n",
      "Learning Rate: 0.0005120000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 32/32 [00:18<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]\n",
      "Train Loss: 0.0063\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7300, Precision: 0.7268, Recall: 0.7300, F1: 0.7258\n",
      "Learning Rate: 0.0004096000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 32/32 [00:18<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]\n",
      "Train Loss: 0.0044\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7300, Precision: 0.7294, Recall: 0.7300, F1: 0.7229\n",
      "Learning Rate: 0.0004096000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 32/32 [00:14<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]\n",
      "Train Loss: 0.0033\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7500, Precision: 0.7451, Recall: 0.7500, F1: 0.7424\n",
      "Learning Rate: 0.0004096000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 32/32 [00:18<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]\n",
      "Train Loss: 0.0028\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7450, Precision: 0.7418, Recall: 0.7450, F1: 0.7372\n",
      "Learning Rate: 0.0003276800000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 32/32 [00:15<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]\n",
      "Train Loss: 0.0028\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7450, Precision: 0.7389, Recall: 0.7450, F1: 0.7359\n",
      "Learning Rate: 0.0003276800000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]\n",
      "Train Loss: 0.0027\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7500, Precision: 0.7474, Recall: 0.7500, F1: 0.7422\n",
      "Learning Rate: 0.0003276800000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 32/32 [00:15<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]\n",
      "Train Loss: 0.0021\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7400, Precision: 0.7371, Recall: 0.7400, F1: 0.7320\n",
      "Learning Rate: 0.0002621440000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 32/32 [00:15<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]\n",
      "Train Loss: 0.0020\n",
      "Train: Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Val: Accuracy: 0.7450, Precision: 0.7423, Recall: 0.7450, F1: 0.7372\n",
      "Learning Rate: 0.0002621440000000001\n",
      "Initializing EvaluationDataset with root_dir: /workspace/data/VGG16_test_features/frontal_view/VGG16_features\n",
      "Found 300 .npy files in /workspace/data/VGG16_test_features/frontal_view/VGG16_features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 19/19 [00:09<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to save results to /workspace/mamba2_test_results.csv\n",
      "Number of file names: 300\n",
      "Number of predictions: 300\n",
      "Class names: ['Left Lane Change', 'Left Turn', 'Right Lane Change', 'Right Turn', 'Slow-Stop', 'Straight']\n",
      "File should have been saved. Does it exist? True\n",
      "File path: /workspace/mamba2_test_results.csv\n",
      "File size: 15103 bytes\n",
      "Testing completed. Results saved to mamba2_test_results.csv\n",
      "Total predictions: 300\n",
      "Unique classes predicted: {0, 1, 2, 3, 4, 5}\n",
      "Class distribution: {3: 37, 5: 75, 2: 37, 1: 92, 0: 19, 4: 40}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    return padded_sequences, torch.stack(labels) if isinstance(labels[0], torch.Tensor) else labels\n",
    "\n",
    "class Mamba2Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, d_state=64, d_conv=4, expand=2):\n",
    "        super(Mamba2Classifier, self).__init__()\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=input_size,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        self.classifier = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.data = [(os.path.join(root_dir, cls, file), self.class_to_idx[cls])\n",
    "                     for cls in self.classes\n",
    "                     for file in os.listdir(os.path.join(root_dir, cls))\n",
    "                     if file.endswith('.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data[idx]\n",
    "        sequence = np.load(file_path, allow_pickle=True)\n",
    "        sequence = (sequence - np.mean(sequence)) / np.std(sequence)\n",
    "        return torch.tensor(sequence, dtype=torch.float32).squeeze(), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_paths = [os.path.join(root_dir, file) for file in os.listdir(root_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        print(len(self.file_paths))\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        sequence = np.load(file_path, allow_pickle=True)\n",
    "        sequence = (sequence - np.mean(sequence)) / np.std(sequence)\n",
    "        return torch.tensor(sequence, dtype=torch.float32).squeeze(), os.path.basename(file_path)\n",
    "\n",
    "\n",
    "class EvaluationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.file_paths = []\n",
    "        \n",
    "        print(f\"Initializing EvaluationDataset with root_dir: {root_dir}\")\n",
    "        \n",
    "        if not os.path.exists(root_dir):\n",
    "            print(f\"Error: The directory {root_dir} does not exist.\")\n",
    "            return\n",
    "        \n",
    "        for file in os.listdir(root_dir):\n",
    "            if file.endswith('.npy'):\n",
    "                full_path = os.path.join(root_dir, file)\n",
    "                self.file_paths.append(full_path)\n",
    "        \n",
    "        print(f\"Found {len(self.file_paths)} .npy files in {root_dir}\")\n",
    "        \n",
    "        if len(self.file_paths) == 0:\n",
    "            print(\"Warning: No .npy files found in the directory.\")\n",
    "            print(\"Contents of the directory:\")\n",
    "            print(os.listdir(root_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        try:\n",
    "            sequence = np.load(file_path, allow_pickle=True)\n",
    "            sequence = (sequence - np.mean(sequence)) / np.std(sequence)\n",
    "            return torch.tensor(sequence, dtype=torch.float32).squeeze(), os.path.basename(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "            return torch.tensor([]), \"\"\n",
    "            \n",
    "def calculate_metrics(loader, model, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "    return (accuracy_score(y_true, y_pred),\n",
    "            precision_score(y_true, y_pred, average='weighted'),\n",
    "            recall_score(y_true, y_pred, average='weighted'),\n",
    "            f1_score(y_true, y_pred, average='weighted'))\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
    "    model.to(device)\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_metrics = calculate_metrics(train_loader, model, device)\n",
    "        val_metrics = calculate_metrics(val_loader, model, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Train: Accuracy: {train_metrics[0]:.4f}, Precision: {train_metrics[1]:.4f}, Recall: {train_metrics[2]:.4f}, F1: {train_metrics[3]:.4f}\")\n",
    "        print(f\"Val: Accuracy: {val_metrics[0]:.4f}, Precision: {val_metrics[1]:.4f}, Recall: {val_metrics[2]:.4f}, F1: {val_metrics[3]:.4f}\")\n",
    "        print(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_metrics[0] > best_accuracy:\n",
    "            best_accuracy = val_metrics[0]\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"New best model saved with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_file_names = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, file_names in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_file_names.extend(file_names)\n",
    "\n",
    "    return all_predictions, all_file_names\n",
    "\n",
    "def save_results_to_csv(file_names, predictions, class_names, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        header = ['frontal_view_video_name'] + class_names\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for file_name, pred in zip(file_names, predictions):\n",
    "            file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "            row = [file_name_without_ext] + [1 if i == pred else 0 for i in range(len(class_names))]\n",
    "            writer.writerow(row)\n",
    "\n",
    "def save_results_to_csv(file_names, predictions, class_names, output_file):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        header = ['frontal_view_video_name'] + class_names\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for file_name, pred in zip(file_names, predictions):\n",
    "            file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "            row = [file_name_without_ext] + [1 if i == pred else 0 for i in range(len(class_names))]\n",
    "            writer.writerow(row)\n",
    "def save_results_to_csv(file_names, predictions, class_names, output_file):\n",
    "       print(f\"Attempting to save results to {output_file}\")\n",
    "       print(f\"Number of file names: {len(file_names)}\")\n",
    "       print(f\"Number of predictions: {len(predictions)}\")\n",
    "       print(f\"Class names: {class_names}\")\n",
    "       \n",
    "       with open(output_file, 'w', newline='') as csvfile:\n",
    "           writer = csv.writer(csvfile)\n",
    "           header = ['frontal_view_video_name'] + class_names\n",
    "           writer.writerow(header)\n",
    "\n",
    "           for file_name, pred in zip(file_names, predictions):\n",
    "               file_name_without_ext = os.path.splitext(file_name)[0]\n",
    "               row = [file_name_without_ext] + [1 if i == pred else 0 for i in range(len(class_names))]\n",
    "               writer.writerow(row)\n",
    "       \n",
    "       print(f\"File should have been saved. Does it exist? {os.path.exists(output_file)}\")\n",
    "       if os.path.exists(output_file):\n",
    "           \n",
    "           print(f\"File path: {os.path.abspath(output_file)}\")\n",
    "           print(f\"File size: {os.path.getsize(output_file)} bytes\")\n",
    "def main():\n",
    "    input_size = 512\n",
    "    num_classes = 6\n",
    "    d_state = 32   #32\n",
    "    d_conv = 4   #4\n",
    "    expand = 8   #2 was the best\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 20\n",
    "    train_dataset = SequenceDataset('/workspace/data/VGG16_Training_Features/frontal_view')\n",
    "    eval_dataset = SequenceDataset('/workspace/data/VGG16_val_features/frontal_view')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    model = Mamba2Classifier(input_size, num_classes, d_state, d_conv, expand)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, eval_loader, criterion, optimizer, scheduler, num_epochs, device)\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    test_dataset = EvaluationDataset('/workspace/data/VGG16_test_features/frontal_view/VGG16_features')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    predictions, file_names = evaluate_model(model, test_loader, device)\n",
    "\n",
    "    class_names = ['Left Lane Change', 'Left Turn', 'Right Lane Change', 'Right Turn', 'Slow-Stop', 'Straight']\n",
    "    save_results_to_csv(file_names, predictions, class_names, '/workspace/mamba2_test_results.csv')\n",
    "\n",
    "    print(\"Testing completed. Results saved to mamba2_test_results.csv\")\n",
    "\n",
    "    from collections import Counter\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Unique classes predicted: {set(predictions)}\")\n",
    "    print(f\"Class distribution: {dict(Counter(predictions))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing EvaluationDataset with root_dir: /workspace/data/VGG16_test_features/frontal_view/VGG16_features\n",
      "Found 0 .npy files in /workspace/data/VGG16_test_features/frontal_view/VGG16_features\n",
      "Warning: No .npy files found in the directory.\n",
      "Contents of the directory:\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m EvaluationDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/data/VGG16_test_features/frontal_view/VGG16_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the testing dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m predictions, file_names \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = EvaluationDataset('/workspace/data/VGG16_test_features/frontal_view/')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluate the model on the testing dataset\n",
    "predictions, file_names = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Left Lane Change', 'Left Turn', 'Right Lane Change', 'Right Turn', 'Slow-Stop', 'Straight']\n",
    "\n",
    "# Save results to CSV\n",
    "save_results_to_csv(file_names, predictions, class_names, '/workspace/mamba2_test_results.csv')\n",
    "\n",
    "print(\"Testing completed. Results saved to mamba2_test_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
