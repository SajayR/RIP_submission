{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import tqdm\n",
        "from mamba_ssm import Mamba2\n",
        "\n",
        "class Mamba2Classifier(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, d_state=64, d_conv=4, expand=2):\n",
        "        super(Mamba2Classifier, self).__init__()\n",
        "        self.mamba = Mamba2(\n",
        "            d_model=input_size,\n",
        "            d_state=d_state,\n",
        "            d_conv=d_conv,\n",
        "            expand=expand\n",
        "        )\n",
        "        self.fc = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mamba(x)\n",
        "        out = out.mean(dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class EnsembleMultiViewMamba2Model(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, d_state=64, d_conv=4, expand=2):\n",
        "        super(EnsembleMultiViewMamba2Model, self).__init__()\n",
        "        self.view_models = nn.ModuleList([\n",
        "            Mamba2Classifier(input_size, num_classes, d_state, d_conv, expand)\n",
        "            for _ in range(3)\n",
        "        ])\n",
        "        self.ensemble_weights = nn.Parameter(torch.ones(3) / 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        view_outputs = []\n",
        "        for i, view_x in enumerate(x):\n",
        "            output = self.view_models[i](view_x)\n",
        "            view_outputs.append(output)\n",
        "\n",
        "        ensemble_weights = F.softmax(self.ensemble_weights, dim=0)\n",
        "        final_output = sum(w * out for w, out in zip(ensemble_weights, view_outputs))\n",
        "        return final_output\n",
        "\n",
        "class MultiViewSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.views = ['frontal_view', 'left_side_mirror_view', 'right_side_mirror_view']\n",
        "        self.classes = sorted(os.listdir(os.path.join(root_dir, self.views[0])))\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        self.data = []\n",
        "\n",
        "        for cls in self.classes:\n",
        "            for file in os.listdir(os.path.join(root_dir, self.views[0], cls)):\n",
        "                if file.endswith('.npy'):\n",
        "                    self.data.append((file, self.class_to_idx[cls]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name, label = self.data[idx]\n",
        "        sequences = []\n",
        "        for view in self.views:\n",
        "            file_path = os.path.join(self.root_dir, view, self.classes[label], file_name)\n",
        "            sequence = np.load(file_path, allow_pickle=True)\n",
        "            sequence = (sequence - np.mean(sequence)) / np.std(sequence)\n",
        "            sequences.append(torch.tensor(sequence, dtype=torch.float32).squeeze())\n",
        "        return sequences, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "class MultiViewTestDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.views = ['frontal_view', 'left_view', 'right_view']\n",
        "        self.file_names = [f for f in os.listdir(os.path.join(root_dir, self.views[0])) if f.endswith('.npy')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_names[idx]\n",
        "        sequences = []\n",
        "        for view in self.views:\n",
        "            file_path = os.path.join(self.root_dir, view, file_name)\n",
        "            sequence = np.load(file_path, allow_pickle=True)\n",
        "            sequence = (sequence - np.mean(sequence)) / np.std(sequence)\n",
        "            sequences.append(torch.tensor(sequence, dtype=torch.float32).squeeze())\n",
        "        return sequences, file_name\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    padded_sequences = [nn.utils.rnn.pad_sequence([seq[i] for seq in sequences], batch_first=True) for i in range(3)]\n",
        "    return padded_sequences, torch.stack(labels) if isinstance(labels[0], torch.Tensor) else labels\n",
        "\n",
        "def calculate_metrics(loader, model, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = [d.to(device) for d in x]\n",
        "            y = y.to(device)\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "    return (accuracy_score(y_true, y_pred),\n",
        "            precision_score(y_true, y_pred, average='weighted'),\n",
        "            recall_score(y_true, y_pred, average='weighted'),\n",
        "            f1_score(y_true, y_pred, average='weighted'))\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n",
        "    model.to(device)\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for data, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            data = [d.to(device) for d in data]\n",
        "            targets = targets.to(device)\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_metrics = calculate_metrics(train_loader, model, device)\n",
        "        val_metrics = calculate_metrics(val_loader, model, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Train: Accuracy: {train_metrics[0]:.4f}, Precision: {train_metrics[1]:.4f}, Recall: {train_metrics[2]:.4f}, F1: {train_metrics[3]:.4f}\")\n",
        "        print(f\"Val: Accuracy: {val_metrics[0]:.4f}, Precision: {val_metrics[1]:.4f}, Recall: {val_metrics[2]:.4f}, F1: {val_metrics[3]:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_metrics[0] > best_accuracy:\n",
        "            best_accuracy = val_metrics[0]\n",
        "            torch.save(model.state_dict(), 'best_multi_view_mamba2_model.pth')\n",
        "            print(f\"New best model saved with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_file_names = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, file_names in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            data = [d.to(device) for d in data]\n",
        "            outputs = model(data)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_file_names.extend(file_names)\n",
        "\n",
        "    return all_predictions, all_file_names\n",
        "\n",
        "def save_results_to_csv(file_names, predictions, class_names, output_file):\n",
        "    with open(output_file, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        header = ['frontal_view_video_name'] + class_names\n",
        "        writer.writerow(header)\n",
        "\n",
        "        for file_name, pred in zip(file_names, predictions):\n",
        "            file_name_without_ext = os.path.splitext(file_name)[0]\n",
        "            row = [file_name_without_ext] + [1 if i == pred else 0 for i in range(len(class_names))]\n",
        "            writer.writerow(row)\n",
        "\n",
        "def main():\n",
        "    input_size = 512\n",
        "    num_classes = 6\n",
        "    d_state = 32\n",
        "    d_conv = 4\n",
        "    expand = 6\n",
        "    batch_size = 16\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 20\n",
        "    train_dataset = MultiViewSequenceDataset('/workspace/data/VGG16_Training_Features')\n",
        "    val_dataset = MultiViewSequenceDataset('/workspace/data/VGG16_val_features')\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    model = EnsembleMultiViewMamba2Model(input_size, num_classes, d_state, d_conv, expand)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.8)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\n",
        "    model.load_state_dict(torch.load('best_multi_view_mamba2_model.pth'))\n",
        "    test_dataset = MultiViewTestDataset('/workspace/data/VGG16_test_features')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    predictions, file_names = evaluate_model(model, test_loader, device)\n",
        "    class_names = ['Left Lane Change', 'Left Turn', 'Right Lane Change', 'Right Turn', 'Slow-Stop', 'Straight']\n",
        "    save_results_to_csv(file_names, predictions, class_names, '/workspace/multiview_test_results.csv')\n",
        "    print(\"Testing completed. Results saved to mamba2_multiview_test_results.csv\")\n",
        "\n",
        "    from collections import Counter\n",
        "    print(f\"Total predictions: {len(predictions)}\")\n",
        "    print(f\"Unique classes predicted: {set(predictions)}\")\n",
        "    print(f\"Class distribution: {dict(Counter(predictions))}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
